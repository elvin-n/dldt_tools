// Copyright (C) 2018-2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <gflags/gflags.h>
#include <iostream>
#include <string>
#include <memory>
#include <vector>
#include <algorithm>
#include <map>
#include <chrono>


#include <samples/common.hpp>
#include <samples/slog.hpp>
#include <samples/args_helper.hpp>

#include "object_detection_sample_ssd.h"

#include "tensorflow/lite/model_builder.h"
#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/interpreter_builder.h"
#include "tensorflow/lite/kernels/register.h"

#if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
#include "tensorflow/lite/delegates/hexagon/hexagon_delegate.h"
#endif

#include <opencv2/opencv.hpp>

typedef std::chrono::high_resolution_clock Time;
typedef std::chrono::nanoseconds ns;


bool ParseAndCheckCommandLine(int argc, char *argv[]) {
    // ---------------------------Parsing and validation of input args--------------------------------------
    gflags::ParseCommandLineNonHelpFlags(&argc, &argv, true);
    if (FLAGS_h) {
        showUsage();
        return false;
    }

    slog::info << "Parsing input parameters" << slog::endl;

    if (FLAGS_i.empty()) {
        throw std::logic_error("Parameter -i is not set");
    }

    if (FLAGS_m.empty()) {
        throw std::logic_error("Parameter -m is not set");
    }

    return true;
}

/**
* \brief The entry point for the Inference Engine object_detection sample application
* \file object_detection_sample_ssd/main.cpp
* \example object_detection_sample_ssd/main.cpp
*/
int main(int argc, char *argv[]) {
    try {
        /** This sample covers certain topology and cannot be generalized for any object detection one **/

        // --------------------------- 1. Parsing and validation of input args ---------------------------------
        if (!ParseAndCheckCommandLine(argc, argv)) {
            return 0;
        }
        // -----------------------------------------------------------------------------------------------------

        // -----------------------------------------------------------------------------------------------------
        // --------------------------- 1. Read IR Generated by SNPE tools (.dl file) ------------
        Time::time_point time1 = Time::now();
        std::unique_ptr<tflite::FlatBufferModel> model;
        model = tflite::FlatBufferModel::BuildFromFile(FLAGS_m.c_str());
        if (!model) {
          std::cerr << "Error while opening the tflite file." << std::endl;
          return EXIT_FAILURE;
        }

        Time::time_point time2 = Time::now();

        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 2. Loading model to the device ------------------------------------------
        std::unique_ptr<tflite::Interpreter> interpreter;
        tflite::ops::builtin::BuiltinOpResolver resolver;

        tflite::InterpreterBuilder(*model, resolver)(&interpreter);
        if (!interpreter) {
          std::cerr << "Failed to construct interpreter." << std::endl;
          return EXIT_FAILURE;
        }
        // interpreter->UseNNAPI(s->old_accel);
        // interpreter->SetAllowFp16PrecisionForFp32(true);

        // forcedly set number of threads as 1 for a while
        interpreter->SetNumThreads(1);

        // there is offloading part
        TfLiteDelegate* delegate = nullptr;
        if (FLAGS_d == "GPU") {
/*        #if defined(__ANDROID__)
          TfLiteGpuDelegateOptionsV2 gpu_opts = TfLiteGpuDelegateOptionsV2Default();
          gpu_opts.inference_preference =
              TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;
          gpu_opts.inference_priority1 =
              s->allow_fp16 ? TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY
                            : TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
          auto delegate = evaluation::CreateGPUDelegate(&gpu_opts);
        #else
          auto delegate = evaluation::CreateGPUDelegate();
        #endif

          if (!delegate) {
            std::cerr << "GPU acceleration is unsupported on this platform.";
            return -1;
          } else {
            delegates.emplace("GPU", std::move(delegate));
          }
*/
        } else if (FLAGS_d == "DSP") {
#if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
          TfLiteHexagonInit();
          TfLiteHexagonDelegateOptions options({0});
          delegate = TfLiteHexagonDelegateCreate(&options);
          if (!delegate) {
            std::cerr << "Hexagon acceleration is unsupported on this platform.";
            TfLiteHexagonTearDown();
            return -1;
          } /*else {
            delegates.emplace("Hexagon", std::move(delegate));
          }*/
#endif
        } else if (FLAGS_d == "CPU") {
            // default execution unit, no additional actions are required
        } else {
           std::cerr << "The device name is not valid. Please select CPU/DSP/GPU." << std::endl;
           return -1;
        }

        if (delegate) {
          if (interpreter->ModifyGraphWithDelegate(delegate) !=
              kTfLiteOk) {
            std::cerr << "Failed to apply TFLite delegate." << std::endl;
            return -1;
          }
          std::cout << "Applied deligation successfully" << std::endl;
        }
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 3. Prepare input --------------------------------------------------------
        size_t imageWidths, imageHeights;
        int input = interpreter->inputs()[0];
        const std::vector<int> inputs = interpreter->inputs();
        if (interpreter->AllocateTensors() != kTfLiteOk) {
          std::cerr << "Failed to allocate tensors!" << std::endl;
          return EXIT_FAILURE;
        }

        // get input dimension from the input tensor metadata
        // assuming one input only
        TfLiteIntArray* dims = interpreter->tensor(input)->dims;
        int wanted_height = dims->data[1];
        int wanted_width = dims->data[2];
        int wanted_channels = dims->data[3];
        cv::Mat image = cv::imread(FLAGS_i);
        imageWidths = image.cols;
        imageHeights = image.rows;

        cv::Mat resized_image(image);
        cv::resize(image, resized_image, cv::Size(wanted_width, wanted_height));

        switch (interpreter->tensor(input)->type) {
          case kTfLiteFloat32:
            {
              // Follow copying of image is not optimal since it happens in two passes
              float *tf = interpreter->typed_tensor<float>(input);
              size_t nielements = wanted_height * wanted_width * wanted_channels;
              for (size_t i = 0; i < nielements; i++) {
                  tf[i] = (static_cast<float>(resized_image.data[i]) - 127.5) / 127.5;
              }

              // reorder from BGR to RGB:
              // snpe-tensorflow-to-dlc has follow parameters --input_encoding "input" bgr --input_type "input" image
              // unfortunately they do not work
              // if they worked, I would not have such code here
              for (size_t i = 0; i < nielements; i += wanted_channels) {
                  float tmp = tf[i];
                  tf[i] = tf[i + 2];
                  tf[i + 2] = tmp;
              }
            }
            break;
          case kTfLiteUInt8:
            {
              uint8_t *tf = interpreter->typed_tensor<uint8_t>(input);
              // reorder from BGR to RGB:
              // snpe-tensorflow-to-dlc has follow parameters --input_encoding "input" bgr --input_type "input" image
              // unfortunately they do not work
              // if they worked, I would not have such code here
              size_t nielements = wanted_height * wanted_width * wanted_channels;
              for (size_t i = 0; i < nielements; i += wanted_channels) {
                  tf[i] = resized_image.data[i + 2];
                  tf[i+1] = resized_image.data[i + 1];
                  tf[i+2] = resized_image.data[i];
              }
            }
            break;
          default:
            std::cerr << "cannot handle model input type!" <<
            interpreter->tensor(input)->type << std::endl;
            return EXIT_FAILURE;
        }
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 4. Do inference --------------------------------------------------------
        /* Running the request synchronously */
        if (interpreter->Invoke() != kTfLiteOk) {
            std::cerr << "Error while executing the network." << std::endl;
            return EXIT_FAILURE;
        }
        Time::time_point time8 = Time::now();

        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 5. Process output ------------------------------------------------------
        slog::info << "Processing output blobs" << slog::endl;
        std::string scoresName = "TFLite_Detection_PostProcess:2";
        std::string classesName = "TFLite_Detection_PostProcess:1";
        std::string boxesName = "TFLite_Detection_PostProcess";
        float* oScores = nullptr;
        float* oClasses = nullptr;
        float* oBoxes = nullptr;
        int nPredictions = 0;

        const std::vector<int> outputs = interpreter->outputs();
        for (size_t o = 0; o < outputs.size(); o++) {
            std::string name = interpreter->GetOutputName(o);
            switch (interpreter->tensor(outputs[o])->type) {
            case kTfLiteFloat32:
                break;
            case kTfLiteUInt8:
                std::cerr << "Do not support U8 data type for '" << name << "' output" << std::endl;
                return EXIT_FAILURE;
                break;
            default:
                std::cerr << "Do not support data type for '" << name << "' output" << std::endl;
                return EXIT_FAILURE;
            }

            if(name == scoresName) {
                oScores =  interpreter->typed_output_tensor<float>(o);
            }
            if(name == classesName) {
                oClasses = interpreter->typed_output_tensor<float>(o);
                TfLiteIntArray* tfdims = interpreter->tensor(outputs[o])->dims;
                nPredictions = tfdims->data[1];
                std::cout << "!!!!!!!!!!!!!!!!!!! " << nPredictions << std::endl;
            }
            if(name == boxesName) {
                oBoxes = interpreter->typed_output_tensor<float>(o);
            }
        }
        if (!oScores || !oClasses || !oBoxes) {
            std::cerr << "Was not able to initialize expected output data for object detectino network" << std::endl;
            return EXIT_FAILURE;
        }

        std::vector<int> boxes;
        std::vector<int> classes;

        for (size_t curProposal = 0; curProposal < nPredictions; curProposal++) {
            float confidence = oScores[curProposal];
            float label = static_cast<int>(oClasses[curProposal]);
            // boxes have follow layout top, left, bottom, right
            // according to this link: https://www.tensorflow.org/lite/models/object_detection/overview
            auto ymin = static_cast<int>(oBoxes[4 * curProposal] * imageHeights);
            auto xmin = static_cast<int>(oBoxes[4 * curProposal + 1] * imageWidths);
            auto ymax = static_cast<int>(oBoxes[4 * curProposal + 2] * imageHeights);
            auto xmax = static_cast<int>(oBoxes[4 * curProposal + 3] * imageWidths);


            std::cout << "[" << curProposal << "," << label << "] element, prob = " << confidence <<
                "    (" << xmin << "," << ymin << ")-(" << xmax << "," << ymax << ")" << " batch id : " << 0;

            if (confidence > 0.5) {
                // Drawing only objects with >50% probability
                classes.push_back(label);
                boxes.push_back(xmin);
                boxes.push_back(ymin);
                boxes.push_back(xmax - xmin);
                boxes.push_back(ymax - ymin);
                std::cout << " WILL BE PRINTED!";
            }
            std::cout << std::endl;
        }

        addRectangles(image.data, imageHeights, imageWidths, boxes, classes,
                      BBOX_THICKNESS);
        const std::string image_path = "out.bmp";
        if (writeOutputBmp(image_path.c_str(), image.data, imageHeights, imageWidths)) {
            slog::info << "Image " + image_path + " created!" << slog::endl;
        } else {
            throw std::logic_error(std::string("Can't create a file: ") + image_path);
        }



        //const float *oData = reinterpret_cast<float *>(&(*outTensor->begin()));
/*


        const Blob::Ptr output_blob = infer_request.GetBlob(outputName);
        MemoryBlob::CPtr moutput = as<MemoryBlob>(output_blob);
        if (!moutput) {
            throw std::logic_error("We expect output to be inherited from MemoryBlob, "
                                   "but by fact we were not able to cast output to MemoryBlob");
        }
        // locked memory holder should be alive all time while access to its buffer happens
        auto moutputHolder = moutput->rmap();
        const float *detection = moutputHolder.as<const PrecisionTrait<Precision::FP32>::value_type *>();

        std::vector<std::vector<int> > boxes(batchSize);
        std::vector<std::vector<int> > classes(batchSize);

        // Each detection has image_id that denotes processed image
        for (int curProposal = 0; curProposal < maxProposalCount; curProposal++) {
            auto image_id = static_cast<int>(detection[curProposal * objectSize + 0]);
            if (image_id < 0) {
                break;
            }

            float confidence = detection[curProposal * objectSize + 2];
            auto label = static_cast<int>(detection[curProposal * objectSize + 1]);
            auto xmin = static_cast<int>(detection[curProposal * objectSize + 3] * imageWidths[image_id]);
            auto ymin = static_cast<int>(detection[curProposal * objectSize + 4] * imageHeights[image_id]);
            auto xmax = static_cast<int>(detection[curProposal * objectSize + 5] * imageWidths[image_id]);
            auto ymax = static_cast<int>(detection[curProposal * objectSize + 6] * imageHeights[image_id]);

            std::cout << "[" << curProposal << "," << label << "] element, prob = " << confidence <<
                "    (" << xmin << "," << ymin << ")-(" << xmax << "," << ymax << ")" << " batch id : " << image_id;

            if (confidence > 0.5) {
                // Drawing only objects with >50% probability
                classes[image_id].push_back(label);
                boxes[image_id].push_back(xmin);
                boxes[image_id].push_back(ymin);
                boxes[image_id].push_back(xmax - xmin);
                boxes[image_id].push_back(ymax - ymin);
                std::cout << " WILL BE PRINTED!";
            }
            std::cout << std::endl;
        }

        for (size_t batch_id = 0; batch_id < batchSize; ++batch_id) {
            addRectangles(originalImagesData[batch_id].get(), imageHeights[batch_id], imageWidths[batch_id], boxes[batch_id], classes[batch_id],
                          BBOX_THICKNESS);
            const std::string image_path = "out_" + std::to_string(batch_id) + ".bmp";
            if (writeOutputBmp(image_path, originalImagesData[batch_id].get(), imageHeights[batch_id], imageWidths[batch_id])) {
                slog::info << "Image " + image_path + " created!" << slog::endl;
            } else {
                throw std::logic_error(std::string("Can't create a file: ") + image_path);
            }
        }
*/
        // -----------------------------------------------------------------------------------------------------
    }
    catch (const std::exception& error) {
        slog::err << error.what() << slog::endl;
        return 1;
    }
    catch (...) {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        return 1;
    }

    slog::info << "Execution successful" << slog::endl;
    slog::info << slog::endl << "This sample is an API example, for any performance measurements "
                                "please use the dedicated benchmark_app tool" << slog::endl;
    return 0;
}
