// Copyright 2020 the dldt tools authors. All rights reserved.
// Use of this source code is governed by a BSD-style license

#include "tflite_backend.hpp"
#if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
#include "tensorflow/lite/delegates/hexagon/hexagon_delegate.h"
#endif

#include <string.h>

bool TFLiteBackend::loadModel(const std::string &model, const std::string &device,
                            const std::vector<std::string> &outputs,
                            const std::map<std::string, std::string> &config) {

    try {

        // --------------------------- 1. Read IR Generated by SNPE tools (.dl file) ------------
        _model = tflite::FlatBufferModel::BuildFromFile(model.c_str());
        if (!_model) {
          // std::cerr << "Error while opening the tflite file." << std::endl;
          return false;
        }

        // --------------------------- 2. Loading model to the device ------------------------------------------
        tflite::ops::builtin::BuiltinOpResolver resolver;

        tflite::InterpreterBuilder(*_model, resolver)(&_interpreter);
        if (!_interpreter) {
          // std::cerr << "Failed to construct interpreter." << std::endl;
          return false;
        }

        _interpreter->SetNumThreads(1);

        // there is offloading part
        TfLiteDelegate* delegate = nullptr;
        if (device == "GPU") {
/*        #if defined(__ANDROID__)
          TfLiteGpuDelegateOptionsV2 gpu_opts = TfLiteGpuDelegateOptionsV2Default();
          gpu_opts.inference_preference =
              TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;
          gpu_opts.inference_priority1 =
              s->allow_fp16 ? TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY
                            : TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
          auto delegate = evaluation::CreateGPUDelegate(&gpu_opts);
        #else
          auto delegate = evaluation::CreateGPUDelegate();
        #endif

          if (!delegate) {
            std::cerr << "GPU acceleration is unsupported on this platform.";
            return -1;
          } else {
            delegates.emplace("GPU", std::move(delegate));
          }
*/
        } else if (device == "DSP") {
#if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
          TfLiteHexagonInit();
          TfLiteHexagonDelegateOptions options({0});
          delegate = TfLiteHexagonDelegateCreate(&options);
          if (!delegate) {
            std::cerr << "Hexagon acceleration is unsupported on this platform.";
            TfLiteHexagonTearDown();
            return -1;
          } /*else {
            delegates.emplace("Hexagon", std::move(delegate));
          }*/
#endif
        } else if (device == "CPU") {
            // default execution unit, no additional actions are required
        } else {
           std::cerr << "The device name is not valid. Please select CPU/DSP/GPU." << std::endl;
           return -1;
        }

        if (delegate) {
          if (_interpreter->ModifyGraphWithDelegate(delegate) !=
              kTfLiteOk) {
            std::cerr << "Failed to apply TFLite delegate." << std::endl;
            return -1;
          }
          std::cout << "Applied deligation successfully" << std::endl;
        }

        // --------------------------- 3. Prepare input --------------------------------------------------------
        if (_interpreter->AllocateTensors() != kTfLiteOk) {
          std::cerr << "Failed to allocate tensors!" << std::endl;
          return EXIT_FAILURE;
        }

        const std::vector<int> inputs = _interpreter->inputs();
        for (size_t i = 0; i < inputs.size(); i++) {
            std::string name = _interpreter->GetInputName(i);

            IOInfo info;
            switch (_interpreter->tensor(inputs[i])->type) {
            case kTfLiteFloat32:
                info._precision = FP32;
                break;
            case kTfLiteUInt8:
                info._precision = U8;
                break;
            default:
                return false;
            }

            TfLiteIntArray* tfdims = _interpreter->tensor(inputs[i])->dims;
            info._shape.resize(tfdims->size);
            for (size_t j = 0; j < tfdims->size; j++) {
                info._shape[j] = tfdims->data[j];
            }
            _inputInfo[name] = info;

            auto vblob = std::make_shared<VBlob>();
            vblob->_precision = info._precision;
            vblob->_shape = info._shape;
            switch (_interpreter->tensor(inputs[i])->type) {
            case kTfLiteFloat32:
                vblob->_data = reinterpret_cast<void *>(_interpreter->typed_tensor<float>(inputs[i]));
                break;
            case kTfLiteUInt8:
                vblob->_data = reinterpret_cast<void *>(_interpreter->typed_tensor<uint8_t>(inputs[i]));
                break;
            default:
                return false;
            }
            vblob->_ownMemory = false;
            vblob->_layout = "NHWC";
            vblob->_colourFormat = "RGB";
            _blobs[name] = vblob;
        }


        const std::vector<int> outputs = _interpreter->outputs();
        for (size_t o = 0; o < outputs.size(); o++) {
            std::string name = _interpreter->GetOutputName(o);

            IOInfo info;
            switch (_interpreter->tensor(outputs[o])->type) {
            case kTfLiteFloat32:
                info._precision = FP32;
                break;
            case kTfLiteUInt8:
                info._precision = U8;
                break;
            default:
                return false;
            }

            TfLiteIntArray* tfdims = _interpreter->tensor(outputs[o])->dims;
            info._shape.resize(tfdims->size);
            for (size_t j = 0; j < tfdims->size; j++) {
                info._shape[j] = tfdims->data[j];
            }
            _outputInfo[name] = info;

            auto vblob = std::make_shared<VBlob>();
            // By fact, there might be (U)int8 output from TFLite model
            // at the same time it is rediculos and eventually we must operate by float output
            // we will convert fron uint8 to float after inference
            vblob->_precision = FP32;
            vblob->_shape = info._shape;
            vblob->_data = (unsigned char *)malloc(product(vblob->_shape) * sizeof(float));
            _blobs[name] = vblob;
        }
    } catch (std::exception &ex) {
        return false;
    }

    return true;
}

void TFLiteBackend::report(const InferenceMetrics &im) const {

}
bool TFLiteBackend::infer() {

    if (_interpreter->Invoke() == kTfLiteOk) {
        const std::vector<int> outputs = _interpreter->outputs();
        for (size_t o = 0; o < outputs.size(); o++) {
            int output = _interpreter->outputs()[o];
            std::string name = _interpreter->GetOutputName(o);
            auto vblob = _blobs[name];
            switch (_interpreter->tensor(output)->type) {
            case kTfLiteFloat32:
                {
                    float* oData = _interpreter->typed_output_tensor<float>(o);
                    memcpy(vblob->_data, oData, product(vblob->_shape) * sizeof(float));
                }
                break;
            case kTfLiteUInt8:
                {
                    uint8_t* oData = _interpreter->typed_output_tensor<uint8_t>(o);
                    float* fblobData = static_cast<float*>(vblob->_data);
                    for (size_t j = 0; j < product(vblob->_shape); j++) {
                        float value = oData[j] + (static_cast<float>(j) / 10000.f);
                        fblobData[j] = value / 256.f;
                    }
                }
                break;
            default:
                std::cerr << "cannot handle network output type " <<
                _interpreter->tensor(output)->type << std::endl;
                return EXIT_FAILURE;
            }
        }
        return true;
    }
    return false;
}

std::shared_ptr<VBlob> TFLiteBackend::getBlob(const std::string &name) {
    return _blobs[name];
}


void TFLiteBackend::release() {
    delete this;
}

VInputInfo TFLiteBackend::getInputDataMap() const {
    return _inputInfo;
}

VOutputInfo TFLiteBackend::getOutputDataMap() const {
    return _outputInfo;
}

Backend* createBackend() {
    return new TFLiteBackend();
}

